{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ben Trevett 의 [Faster Sentiment Analysis](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/3%20-%20Faster%20Sentiment%20Analysis.ipynb) 튜토리얼을 한글 데이터셋에 적용해보는 연습이다. 데이터셋은 [네이버 영화 평점 데이터](https://github.com/e9t/nsmc)을 이용한다.\n",
    "\n",
    "이 튜토리얼에서는 `FastText` 모델을 이용해서 모델을 경량화해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리\n",
    "\n",
    "`FastText` 논문의 핵심 아이디어 중 하나는 입력 문장의 마지막에 문장 구성 토큰들의 n-gram을 추가로 도입하는 것이다. 우리는 여기서 bi-gram을 도입하자. 예를 들어 \"how are you ?\"의 bi-gram은 \"how are\", \"are you\" and \"you ?\"이다.\n",
    "\n",
    "따라서 여기서 `generate_bigrams` 함수를 도입하여 토큰화된 문장의 뒤에 bi-gram을 추가하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigrams(x):\n",
    "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
    "    for n_gram in n_grams:\n",
    "        x.append(' '.join(n_gram))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('너', '임마'), ('먹고', '다니냐'), ('밥은', '먹고'), ('임마', '밥은')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ['너', '임마', '밥은', '먹고', '다니냐']\n",
    "n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
    "n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['너', '임마', '밥은', '먹고', '다니냐', '임마 밥은', '밥은 먹고', '먹고 다니냐', '너 임마']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_bigrams(['너', '임마', '밥은', '먹고', '다니냐'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchtext의 `Field`는 `preprocessing` 과정이 있어서 여기에 함수를 전달하면 토크나이징 후 적용된다. 여기에 `generate_bigrams` 함수를 넣자.\n",
    "\n",
    "우리는 한글 데이터를 다루므로 토크나이저 또한 별도로 지정해야한다. 여기서는 [KoNLPy](https://konlpy-ko.readthedocs.io/ko/v0.4.3/)의 은전한닢 tokenizer를 이용한다. 또한 패딩을 추가한다. 여기서는 RNN 안쓸 거기 때문에 packed padded seq.를 쓸 수 없어서 `include_lengths=True` 또한 넣을 필요가 없다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = mecab.morphs, preprocessing = generate_bigrams)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리된 네이버 영화 평점 데이터를 불러오고 검증 데이터를 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {'text': ('text',TEXT), 'label': ('label',LABEL)}\n",
    "# dictionary 형식은 {csv컬럼명 : (데이터 컬럼명, Field이름)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.TabularDataset.splits(\n",
    "                            path = 'data',\n",
    "                            train = 'train_data.csv',\n",
    "                            test = 'test_data.csv',\n",
    "                            format = 'csv',\n",
    "                            fields = fields,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 단어 벡터는 전처리된 단어 벡터를 받자. 원 튜토리얼에선 `glove.100d`를 쓰지만 이건 한글을 지원하지 않으므로, 여기선 한글을 지원하는 `fasttext.simple.300d` 를 사용하겠다. 그리고 사전훈련된 단어집에 없는 단어는 0으로 처리하는 걸 방지하기 위해 `unk_init = torch.Tensor.normal_` 옵션을 준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(train_data,\n",
    "                max_size = MAX_VOCAB_SIZE,\n",
    "                vectors = 'fasttext.simple.300d',\n",
    "                unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '.', '이', '는']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None, {'0': 0, '1': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['우리',\n",
       "  '나라',\n",
       "  '에',\n",
       "  '이런',\n",
       "  '영화',\n",
       "  '가',\n",
       "  '더',\n",
       "  '이상',\n",
       "  '나오',\n",
       "  '지',\n",
       "  '않',\n",
       "  '았',\n",
       "  '으면',\n",
       "  '.',\n",
       "  '..',\n",
       "  '가 더',\n",
       "  '나오 지',\n",
       "  '이런 영화',\n",
       "  '지 않',\n",
       "  '않 았',\n",
       "  '더 이상',\n",
       "  '나라 에',\n",
       "  '영화 가',\n",
       "  '이상 나오',\n",
       "  '았 으면',\n",
       "  '으면 .',\n",
       "  '. ..',\n",
       "  '에 이런',\n",
       "  '우리 나라'],\n",
       " 'label': '0'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data.examples[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 생성자를 만들자. 한글 데이터에선 오류가 발생해서 아래와 같이 `sort_key = lambda x: len(x.text)` 문장을 먼저 넣어줘야 오류없이 작동한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2533,   360,  3591,  ...,   163,   443, 14207],\n",
       "        [   54,     3,   103,  ...,   530,   109, 14207],\n",
       "        [ 2647,   186,    15,  ...,   515,   426,   556],\n",
       "        ...,\n",
       "        [    1,     1,     1,  ...,     1,     1,     1],\n",
       "        [    1,     1,     1,  ...,     1,     1,     1],\n",
       "        [    1,     1,     1,  ...,     1,     1,     1]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iterator)).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('케이블', '에서', '밤')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[2533], TEXT.vocab.itos[54], TEXT.vocab.itos[2647]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('머니', '머니', '해도')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[14207], TEXT.vocab.itos[14207], TEXT.vocab.itos[556]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sent_len * batch_size] 형태로 이루어져 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 생성\n",
    "\n",
    "여기서는 입력 문장을 임베딩 시킨 후 평균을 취한 다음 행렬곱을 취하는 모델을 사용한다. RNN을 사용하지 않기 때문에 파라미터 수가 훨씬 줄어들었다.\n",
    "\n",
    "<img src = 'https://github.com/bentrevett/pytorch-sentiment-analysis/raw/79bb86abc9e89951a5f8c4a25ca5de6a491a4f5d/assets/sentiment8.png'>\n",
    "\n",
    "평균은 다음과 같은 방식으로 취하며 `nn.functional.avg_pool2d` 를 사용한다. 이 함수는 2차원 튜플을 인수로 받으며, 입력 데이터의 마지막 2개 차원을 이용하여 평균을 산출한다.\n",
    "\n",
    "<img src = 'https://github.com/bentrevett/pytorch-sentiment-analysis/raw/79bb86abc9e89951a5f8c4a25ca5de6a491a4f5d/assets/sentiment10.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이즈 계산을 위한 함수를 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape(name, data):\n",
    "    print(f'{name} has shape {data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일단 `avg_pool2d` 에 대해 ARABOZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -> Tensor\n",
       "\n",
       "Applies 2D average-pooling operation in :math:`kH \\times kW` regions by step size\n",
       ":math:`sH \\times sW` steps. The number of output features is equal to the number of\n",
       "input planes.\n",
       "\n",
       "See :class:`~torch.nn.AvgPool2d` for details and output shape.\n",
       "\n",
       "Args:\n",
       "    input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n",
       "    kernel_size: size of the pooling region. Can be a single number or a\n",
       "      tuple `(kH, kW)`\n",
       "    stride: stride of the pooling operation. Can be a single number or a\n",
       "      tuple `(sH, sW)`. Default: :attr:`kernel_size`\n",
       "    padding: implicit zero paddings on both sides of the input. Can be a\n",
       "      single number or a tuple `(padH, padW)`. Default: 0\n",
       "    ceil_mode: when True, will use `ceil` instead of `floor` in the formula\n",
       "        to compute the output shape. Default: ``False``\n",
       "    count_include_pad: when True, will include the zero-padding in the\n",
       "        averaging calculation. Default: ``True``\n",
       "    divisor_override: if specified, it will be used as divisor, otherwise\n",
       "         size of the pooling region will be used. Default: None\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.functional.avg_pool2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 10]), torch.Size([2, 1, 10]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = torch.rand(2,5,10)\n",
    "txt.shape, F.avg_pool2d(txt, (5,1)).shape\n",
    "# (5 x 1) 크기의 필터를 옮겨가며 평균을 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 4]) \n",
      " tensor([[[1., 2., 3., 4.],\n",
      "         [4., 5., 6., 7.]]])\n"
     ]
    }
   ],
   "source": [
    "txt = torch.tensor(\n",
    "    [[[1,2,3,4],[4,5,6,7]]], dtype=torch.float\n",
    ")\n",
    "print(txt.shape,\"\\n\", txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 4]), tensor([[[2.5000, 3.5000, 4.5000, 5.5000]]]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.avg_pool2d(txt, (2,1)).shape, F.avg_pool2d(txt, (2,1))\n",
    "# (2 x 1) 필터로 평균을 취함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 2]), tensor([[[3., 5.]]]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.avg_pool2d(txt, (2,2)).shape, F.avg_pool2d(txt, (2,2))\n",
    "# (2 x 2) 필터로 평균을 취함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FastText` 모델을 구현하자. 다만 여기서 주의할 점!\n",
    "\n",
    "* RNN은 [sent_len, batch_size, embedding_dim] 크기의 텐서를 입력으로 받음\n",
    "* CNN은 [batch_size, sent_len, embedding_dim] 크기의 텐서를 입력으로 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text = [sent_len, batch_size]\n",
    "        #print_shape('text', text)\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        #print_shape('embedded', embedded)\n",
    "        # embedded = [sent_len, batch_size, embedding_dim]\n",
    "        \n",
    "        # CNN은 [batch_size, sent_len, embedding_dim] 를 입력으로 받음\n",
    "        # 따라서 permute 취해줘야 함\n",
    "        embedded = embedded.permute(1,0,2)\n",
    "        #print_shape('embedded', embedded)\n",
    "        # embedded = [batch_size, sent_len, embedding_dim]\n",
    "        \n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1],1)).squeeze(1)\n",
    "        #print_shape('pooled', pooled)\n",
    "        # pooled = [batch_size, embedding_dim]\n",
    "        \n",
    "        res = self.fc(pooled)\n",
    "        #print_shape('res', res)\n",
    "        # res = [batch_size, output_dim]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이즈 계산 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inp = next(iter(train_iterator))\n",
    "#model(inp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 하이퍼파라미터를 설정하고 인스턴스화 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "OUTPUT_DIM = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 파라미터 갯수는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 파라미터 수는 7,500,901 개 입니다.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'모델의 파라미터 수는 {count_parameters(model):,} 개 입니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지난 모델에 비해 약 3/4 으로 감소했다는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사전 훈련된 단어 벡터를 덮어 쓰자. 먼저 텐서 차원을 비교해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 300]) torch.Size([25002, 300])\n"
     ]
    }
   ],
   "source": [
    "pretrained_weight = TEXT.vocab.vectors\n",
    "print(pretrained_weight.shape, model.embedding.weight.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ..., -1.4447,  0.8402, -0.8668],\n",
       "        [ 0.1032, -1.6268,  0.5729,  ...,  0.3180, -0.1626, -0.0417],\n",
       "        [ 0.0569, -0.0520,  0.2733,  ..., -0.0695, -0.1606, -0.0989],\n",
       "        ...,\n",
       "        [ 0.2542,  1.2173,  1.8023,  ..., -0.9746,  0.1054, -1.7293],\n",
       "        [-0.1084, -0.5668,  0.1102,  ..., -0.5685,  1.6376,  0.2508],\n",
       "        [-0.3535,  1.0225, -1.7970,  ...,  0.0683,  0.3403,  1.5236]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`UNK_IDX`와 `PAD_IDX`는 제로 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 훈련\n",
    "\n",
    "이전과 동일하게 하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds==y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 함수를 정의하자. 여기선 드랍아웃 안쓰지만 걍 `model.train()` 사용하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.text).squeeze(1) # output_dim = 1\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "얼마나 훈련 걸리는 지 체크하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 훈련시켜보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.442 | Train Acc: 80.87%\n",
      "\t Val. Loss: 0.360 |  Val. Acc: 85.12%\n",
      "Epoch: 02 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.315 | Train Acc: 87.39%\n",
      "\t Val. Loss: 0.342 |  Val. Acc: 86.13%\n",
      "Epoch: 03 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.277 | Train Acc: 89.06%\n",
      "\t Val. Loss: 0.345 |  Val. Acc: 86.28%\n",
      "Epoch: 04 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.254 | Train Acc: 90.07%\n",
      "\t Val. Loss: 0.354 |  Val. Acc: 86.31%\n",
      "Epoch: 05 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.238 | Train Acc: 90.66%\n",
      "\t Val. Loss: 0.368 |  Val. Acc: 85.94%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나쁘지 않군! 게다가 훈련 시간은 1/4 이다.\n",
    "\n",
    "테스트셋에서 돌려보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.345 | Test Acc: 86.18%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut3-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 훈련시켜보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.275 | Train Acc: 89.17%\n",
      "\t Val. Loss: 0.345 |  Val. Acc: 86.28%\n",
      "Epoch: 07 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.253 | Train Acc: 90.11%\n",
      "\t Val. Loss: 0.356 |  Val. Acc: 86.16%\n",
      "Epoch: 08 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.238 | Train Acc: 90.68%\n",
      "\t Val. Loss: 0.368 |  Val. Acc: 85.99%\n",
      "Epoch: 09 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.226 | Train Acc: 91.16%\n",
      "\t Val. Loss: 0.385 |  Val. Acc: 85.60%\n",
      "Epoch: 10 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.216 | Train Acc: 91.56%\n",
      "\t Val. Loss: 0.400 |  Val. Acc: 85.50%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+6:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오버피팅이 발생하고 있다..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.345 | Test Acc: 86.18%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut3-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능은 이전 모델과 거의 비슷하지만 훈련 시간이 대폭 감소!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 데이터 사용\n",
    "\n",
    "영화 평가 데이터 직접 넣어보자.\n",
    "\n",
    "다음 기능을 하는 `predict_sentiment` 함수를 만들자.\n",
    "\n",
    "* sets the model to evaluation mode\n",
    "* tokenizes the sentence, i.e. splits it from a raw string into a list of tokens\n",
    "* indexes the tokens by converting them into their integer representation from our vocabulary\n",
    "* gets the length of our sequence\n",
    "* converts the indexes, which are a Python list into a PyTorch tensor\n",
    "* add a batch dimension by unsqueezeing\n",
    "* converts the length into a tensor\n",
    "* squashes the output prediction from a real number between 0 and 1 with the `sigmoid` function\n",
    "* converts the tensor holding a single value into an integer with the item() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = generate_bigrams([tok for tok in mecab.morphs(sentence)])\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1) # 배치 \n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9986088871955872"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"이 영화 진짜 재밌었다!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0041242544539272785"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"영화관에서 이걸 본 내가 바보다. 내 돈 돌려줘!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020292015746235847"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"이 영화 감독 밥은 먹고 다니냐? 이런 영화 만들고 잠이 와?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9285458922386169"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"내 인생 영화 등극. 주인공한테 너무 몰입해서 시간 가는 줄도 몰랐다...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

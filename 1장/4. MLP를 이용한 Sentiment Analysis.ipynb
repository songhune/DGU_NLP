{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To Use NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MLP를 이용한 Sentiment Analysis\n",
    "본 강의서 사용된 자료출처: https://github.com/e9t/nsmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import find\n",
    "import gensim\n",
    "#import tensorflow as tf\n",
    "\n",
    "root_dir = \"../data\"\n",
    "train_file = root_dir + '/ratings_train-Copy1.txt'\n",
    "test_file = root_dir+ '/ratings_train-Copy1.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_file, delimiter = '\\t')\n",
    "test = pd.read_csv(test_file, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.label.values\n",
    "y_test = train.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      1\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 라벨: 부정평가(0), 긍정평가(1) 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 0 1 0]\n",
      "150000\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                      9976970\n",
       "document    아 더빙.. 진짜 짜증나네요 목소리\n",
       "label                         0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter, Komoran\n",
    "from time import strftime\n",
    "import time\n",
    "twitter = Twitter()\n",
    "komoran = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 더빙.. 진짜 짜증나네요 목소리\n",
      "['아', '더빙', '..', '진짜', '짜증나다', '목소리']\n",
      "['아', '더빙', '.', '.', '진짜', '짜증', '나', '네요', '목소리']\n",
      "[('아', 'Exclamation'), ('더빙', 'Noun'), ('..', 'Punctuation'), ('진짜', 'Noun'), ('짜증나네요', 'Adjective'), ('목소리', 'Noun')]\n",
      "[('아', 'IC'), ('더빙', 'NNP'), ('.', 'SF'), ('.', 'SF'), ('진짜', 'MAG'), ('짜증', 'NNG'), ('나', 'VV'), ('네요', 'EC'), ('목소리', 'NNG')]\n"
     ]
    }
   ],
   "source": [
    "print(train.loc[0, 'document'])\n",
    "print(twitter.morphs(train.loc[0, 'document'], stem=True, norm=True))\n",
    "print(komoran.morphs(train.loc[0, 'document']))\n",
    "print(twitter.pos(train.loc[0, 'document']))\n",
    "print(komoran.pos(train.loc[0, 'document']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 형태소분석-train/test set의 morpheme을 떼어내 저장\n",
    "train/test_segementation : 각 단어의 형태소만을 떼어내 저장한 것  \n",
    "train/test_sentence: 1문장씩 들어 있는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(sequental_data):\n",
    "    segmentation  = []\n",
    "    for i in range(len(sequental_data)):\n",
    "        if isinstance(sequental_data.loc[i, 'document'], float):\n",
    "            continue\n",
    "        tokens = komoran.morphs(test.loc[i,'document'])\n",
    "        segmentation.append(tokens)\n",
    "    return segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(segmented_data):\n",
    "    sentence = []\n",
    "    for i in range(len(segmented_data)):\n",
    "        temp = ''\n",
    "        for j in range(len(segmented_data[i])-1):\n",
    "            temp += train_segmentation[i][j] + ' '\n",
    "        temp += segmented_data[i][len(segmented_data[i])-1]\n",
    "        sentence.append(temp)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 525.0615768432617 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Twitter Speed calc\n",
    "train_segmentation  = []\n",
    "start_time = time.time()\n",
    "for i in range(len(train)):\n",
    "    if isinstance(train.loc[i, 'document'], float):\n",
    "        continue\n",
    "    tokens = twitter.morphs(train.loc[i,'document'], norm=True, stem=True)\n",
    "    train_segmentation.append(tokens)\n",
    "print(\"--- %s seconds ---\" %(time.time() - start_time))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 103.13204789161682 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Komoran Speed calc - 코모란을 씁시다\n",
    "train_segmentation  = []\n",
    "start_time = time.time()\n",
    "for i in range(len(train.index)):\n",
    "    if isinstance(train.loc[i, 'document'], float):\n",
    "        continue\n",
    "    tokens = komoran.morphs(train.loc[i,'document'])\n",
    "    train_segmentation.append(tokens)\n",
    "print(\"--- %s seconds ---\" %(time.time() - start_time))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_segmentation = segmentation(train)\n",
    "test_segmentation  = segmentation(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence = parsing(train_segmentation)\n",
    "test_sentence = parsing(test_segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/numpy/core/_asarray.py:171: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "#만들어놓은 list들 저장\n",
    "np.save('../data/train_segmentation', train_segmentation)\n",
    "np.save('../data/test_segmentation', test_segmentation)\n",
    "np.save('../data/train_sentence', train_sentence)\n",
    "np.save('../data/test_sentence', test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아', '더빙', '.', '.', '진짜', '짜증', '나', '네요', '목소리']\n"
     ]
    }
   ],
   "source": [
    "#이렇게 만들어놓은 np 데이터는 load명령어로 불러옵니다.\n",
    "train_segmentation = np.load('../data/train_segmentation.npy',allow_pickle=True)\n",
    "train_sentence = np.load('../data/train_sentence.npy',allow_pickle=True)\n",
    "test_segmentation = np.load('../data/test_segmentation.npy',allow_pickle=True)\n",
    "print(train_segmentation[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `gensim.models.Word2Vec()` not found.\n"
     ]
    }
   ],
   "source": [
    "?gensim.models.Word2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(train_segmentation,min_count=2, window=5, vector_size=300, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('./data/sentiment.bin')\n",
    "#모델 불러오기\n",
    "model = gensim.models.Word2Vec.load('../data/sentiment.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors=model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.5 Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['아', '더빙', '.', '.', '진짜', '짜증', '나', '네요', '목소리'])] \n",
      "\n",
      " <class 'numpy.ndarray'> \n",
      "\n",
      "\n",
      "['아 더빙 . . 진짜 짜증 나 네요 목소리'] \n",
      "\n",
      " <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(train_segmentation[:1],'\\n\\n',type(train_segmentation),'\\n\\n')\n",
    "print(train_sentence[:1],'\\n\\n',type(train_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **단어장 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아', '더빙', '.', '.', '진짜', '짜증', '나', '네요', '목소리', '흠']\n",
      "2865517\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "word_dict =[]\n",
    "for sentences in train_segmentation:\n",
    "    for segmentation in sentences:\n",
    "        word_dict.append(segmentation)\n",
    "print(word_dict[:10])\n",
    "print(len(word_dict))\n",
    "print(type(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60400\n",
      "<class 'dict'>\n",
      "(1, '화원')\n",
      "(2, '알랑드롱을')\n",
      "(3, '그냥너랑봐서좋앗따ㅡ')\n",
      "(4, '샬리즈테론')\n",
      "(5, '감동적이다ㅠ')\n"
     ]
    }
   ],
   "source": [
    "word2num = {word:(index+1) for index,word in enumerate(set(word_dict))} \n",
    "num2word = {(index+1):word for index,word in enumerate(set(word_dict))}\n",
    "print(len(word2num))\n",
    "print(type(word2num))\n",
    "for i in range(5):\n",
    "      print(list(num2word.items())[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어를 Vectorize하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = [np.zeros(shape=300)]\n",
    "for index, word in enumerate(word2num.keys()):\n",
    "    if word not in vectors.key_to_index:\n",
    "        embedding_vector += [np.zeros(shape=300)] #[np.random.normal(scale=1e-2, size=300)] 이걸 왜 넣었지?\n",
    "    else:\n",
    "        embedding_vector += [vectors[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2num['<UNK>'] = 0\n",
    "num2word[0] = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  sentence2index (segment):\n",
    "    idx = []\n",
    "    for sentence in segment:\n",
    "        temp = []\n",
    "        for segment in sentence:\n",
    "            if segment not in word2num.keys():\n",
    "                segment = '<UNK>'\n",
    "            temp.append(word2num[segment])\n",
    "        idx.append(temp)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15710, 12884, 41808, 41808, 28939, 20602, 13046, 10196, 19217]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx = sentence2index(train_segmentation)\n",
    "test_idx = sentence2index(test_segmentation)\n",
    "train_idx[:1]\n",
    "test_idx[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2EmbedSum(idxSet, embedding_vector):\n",
    "    sum_w2v = []\n",
    "    for idxes in idxSet:\n",
    "        temp = np.zeros(shape=300)\n",
    "        for idx in idxes:\n",
    "            temp += embedding_vector[idx]\n",
    "        sum_w2v.append(temp)\n",
    "    return sum_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = idx2EmbedSum(train_idx, embedding_vector)\n",
    "X_test_w2v = idx2EmbedSum(test_idx, embedding_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test_w2v[999]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 학습및 테스트하기 (Legacy)_2장에 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Tensorflow 기본 동작 원리 -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../data/pic/3.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 2 #0과 1이므로\n",
    "embedding = 300 #보통 Word2Vec형태의 임베딩, 300차원으로 한다.\n",
    "hidden = 128 #hidden layer을 지날때의 벡터 차원, 아무렇게 바꿔줘도 된다.\n",
    "learning_rate = 1e-3 #learning rate, 케이스에 따라 다르긴 한데, 보통 0.01에서 시작한다.\n",
    "epoch = 10 #몇 번 돌 것인가\n",
    "batch_size = 64 #batch norm을 할때 한번에 읽어들일 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(0)\n",
    "#Placeholder\n",
    "X = tf.placeholder(tf.float32, shape=[None, embedding]) #그래프를 그릴땐, 몇개인지 정의하지 않는다, 300차원\n",
    "Y = tf.placeholder(tf.int32, shape=None) #정답, 0과 1로만 되어있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight, activation\n",
    "W1 = tf.Variable(tf.truncated_normal([embedding, hidden]))\n",
    "b1 = tf.Variable(tf.truncated_normal([hidden]))\n",
    "activate = tf.nn.relu(tf.matmul(X, W1)+b1)#보통 nlp에서 activation function은 hyperbolantic tangent를 쓴다\n",
    "W2 = tf.Variable(tf.truncated_normal([hidden, output]))\n",
    "b2 = tf.Variable(tf.truncated_normal([output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost 계산\n",
    "logits = tf.matmul(activate, W2) + b2\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "prediction = tf.cast(tf.argmax(hypothesis, 1), tf.int32)\n",
    "correct = tf.equal(prediction, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train\n",
    "for epoch in range(epoch):\n",
    "    total_batch = int(len(y_train) / batch_size)\n",
    "    cost_avg = 0\n",
    "    print('< epoch :', (epoch+1), '>')\n",
    "    for i in range(total_batch):\n",
    "        if i == (total_batch-1):\n",
    "            batch_xs = X_train_w2v[(i*batch_size):len(y_train)]\n",
    "            batch_ys = y_train[(i*batch_size):len(y_train)]\n",
    "        else:\n",
    "            batch_xs = X_train_w2v[i*batch_size:(i+1)*batch_size]\n",
    "            batch_ys = y_train[i*batch_size:(i+1)*batch_size]       \n",
    "        cost_val, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "        cost_avg += cost_val\n",
    "        if i % 1000 == 999:\n",
    "            print('%d' % (i+1), 'Cost: ', '{:.3f}'.format(cost_avg/1000))\n",
    "            cost_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_batch = int(len(X_test_w2v) / batch_size)\n",
    "test_acc = 0\n",
    "for i in range(test_batch):\n",
    "    if i == (test_batch-1):\n",
    "        batch_xs = X_test_w2v[(i*batch_size):len(X_test_w2v)]\n",
    "        batch_ys = y_test[(i*batch_size):y_test.shape[0]]\n",
    "    else:\n",
    "        batch_xs = X_test_w2v[i*batch_size:(i+1)*batch_size]\n",
    "        batch_ys = y_test[i*batch_size:(i+1)*batch_size]       \n",
    "    acc = sess.run(accuracy, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "    test_acc += acc\n",
    "print('Accuracy: ', '{:.3f}'.format(test_acc/test_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
